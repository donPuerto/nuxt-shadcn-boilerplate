# Default rules for all crawlers
User-agent: *
Allow: /

# Disallow crawling of administrative and private paths
Disallow: /admin/
Disallow: /private/
Disallow: /api/
Disallow: /_nuxt/
Disallow: /.nuxt/
Disallow: /dist/
Disallow: /.git/

# Prevent crawling of asset files
Disallow: /*.json$
Disallow: /*.js$
Disallow: /*.css$
Disallow: /*.map$

# Crawl-delay to prevent server overload
Crawl-delay: 10

# Sitemap location
Sitemap: https://your-domain.com/sitemap.xml

# Additional rules for specific bots
User-agent: GPTBot
Disallow: /

User-agent: CCBot
Disallow: /
